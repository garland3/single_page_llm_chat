# single_page_llm_chat
 A single page LLM chat inferface. 

![image-alt-text](screen1.png)


![image-alt-text](screen2.png)


For ollama, since it is running on my local host, I had to allow CORS. 

![image-alt-text](screen3.png)
cors-plugin by Raghudevan


I also had to set OLLAMA_ORIGINS="*" on my server before running ollama serve
